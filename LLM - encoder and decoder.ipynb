{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOTPU82B/KiDtH4BPoHDT5W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random\n","import pandas as pd\n","import kagglehub"],"metadata":{"id":"gf79RVfcwrjw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"devicharith/language-translation-englishfrench\")\n","\n","print(\"Path to dataset files:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jBsZn3TY7Mvb","executionInfo":{"status":"ok","timestamp":1769575918801,"user_tz":-330,"elapsed":1772,"user":{"displayName":"Alby J M","userId":"03291698004133179097"}},"outputId":"2912a473-f8fe-4301-8dd9-e4e8a49b553d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using Colab cache for faster access to the 'language-translation-englishfrench' dataset.\n","Path to dataset files: /kaggle/input/language-translation-englishfrench\n"]}]},{"cell_type":"code","source":["file_path = os.path.join(path, \"eng_-french.csv\")\n","\n","with open(file_path, \"r\", encoding=\"utf-8\") as f:\n","    pairs = f.readlines()"],"metadata":{"id":"AEKNpshO7dfU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# synthetic dataset\n","pairs = [\n","    (\"input1\", \"translated_output1\"),\n","    (\"input2\", \"translated_output2\"),\n","    (\"input3\", \"translated_output3\"),\n","    (\"input4\", \"translated_output4\"),\n","    (\"input5\", \"translated_output5\"),\n","    (\"input6\", \"translated_output6\"),\n","]"],"metadata":{"id":"kzzQLk7kxtbX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["PAD = \"<pad>\"\n","SOS = \"<sos>\"\n","EOS = \"<eos>\"\n","\n","def tokenize(s):\n","    return s.lower().split()\n","\n","class Vocab:\n","    def __init__(self):\n","        self.stoi = {PAD:0, SOS:1, EOS:2}\n","        self.itos = {0:PAD, 1:SOS, 2:EOS}\n","\n","    def build(self, sentences):\n","        idx = 3\n","        for s in sentences:\n","            for tok in tokenize(s):\n","                if tok not in self.stoi:\n","                    self.stoi[tok] = idx\n","                    self.itos[idx] = tok\n","                    idx += 1\n","\n","    def encode(self, s):\n","        return [self.stoi[t] for t in tokenize(s)]"],"metadata":{"id":"3RdY4AO4xulC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["src_vocab = Vocab()\n","tgt_vocab = Vocab()\n","\n","src_vocab.build([p[0] for p in pairs])\n","tgt_vocab.build([p[1] for p in pairs])\n","\n","def make_tensors(pairs):\n","    data = []\n","    for src, tgt in pairs:\n","        src_ids = src_vocab.encode(src)\n","        tgt_ids = [tgt_vocab.stoi[SOS]] + tgt_vocab.encode(tgt) + [tgt_vocab.stoi[EOS]]\n","        data.append((src_ids, tgt_ids))\n","    return data\n","\n","data = make_tensors(pairs)"],"metadata":{"id":"X1c91MwMxvsZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim):\n","        super().__init__()\n","        self.emb = nn.Embedding(vocab_size, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n","\n","    def forward(self, x):\n","        emb = self.emb(x)\n","        _, (h, c) = self.lstm(emb)\n","        return h, c"],"metadata":{"id":"_M0IWt3Pxw2F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim):\n","        super().__init__()\n","        self.emb = nn.Embedding(vocab_size, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, vocab_size)\n","\n","    def forward(self, x, h, c):\n","        emb = self.emb(x)\n","        out, (h, c) = self.lstm(emb, (h, c))\n","        logits = self.fc(out)\n","        return logits, h, c"],"metadata":{"id":"Duy0e5sWxyjh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder = Encoder(len(src_vocab.stoi), 64, 128)\n","decoder = Decoder(len(tgt_vocab.stoi), 64, 128)\n","\n","criterion = nn.CrossEntropyLoss()\n","enc_opt = optim.Adam(encoder.parameters(), lr=0.01)\n","dec_opt = optim.Adam(decoder.parameters(), lr=0.01)\n","\n","for epoch in range(300):\n","    total_loss = 0\n","    random.shuffle(data)\n","\n","    for src_ids, tgt_ids in data:\n","        src = torch.tensor([src_ids])\n","        tgt = torch.tensor([tgt_ids])\n","\n","        enc_opt.zero_grad()\n","        dec_opt.zero_grad()\n","\n","        # ---- ENCODER ----\n","        h, c = encoder(src)\n","\n","        # ---- DECODER ----\n","        loss = 0\n","        x = tgt[:, 0].unsqueeze(1)  # <sos>\n","\n","        for t in range(1, tgt.size(1)):\n","            logits, h, c = decoder(x, h, c)\n","            loss += criterion(logits.squeeze(1), tgt[:, t])\n","            x = tgt[:, t].unsqueeze(1)  # teacher forcing\n","\n","        loss.backward()\n","        enc_opt.step()\n","        dec_opt.step()\n","\n","        total_loss += loss.item()\n","\n","    if epoch % 50 == 0:\n","        print(f\"Epoch {epoch} | Loss: {total_loss:.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"atu5H9zxx0eP","executionInfo":{"status":"ok","timestamp":1769506254196,"user_tz":-330,"elapsed":10483,"user":{"displayName":"Alby J M","userId":"03291698004133179097"}},"outputId":"bcb99030-b399-4117-d5e5-5c2b1d69d168"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 | Loss: 24.12\n","Epoch 50 | Loss: 0.00\n","Epoch 100 | Loss: 0.00\n","Epoch 150 | Loss: 0.00\n","Epoch 200 | Loss: 0.00\n","Epoch 250 | Loss: 0.00\n"]}]},{"cell_type":"code","source":["def translate(sentence):\n","    encoder.eval()\n","    decoder.eval()\n","\n","    src_ids = src_vocab.encode(sentence)\n","    src = torch.tensor([src_ids])\n","\n","    with torch.no_grad():\n","        h, c = encoder(src)\n","\n","    x = torch.tensor([[tgt_vocab.stoi[SOS]]])\n","    result = []\n","\n","    for _ in range(10):\n","        with torch.no_grad():\n","            logits, h, c = decoder(x, h, c)\n","        pred = logits.argmax(-1).item()\n","\n","        if pred == tgt_vocab.stoi[EOS]:\n","            break\n","\n","        result.append(tgt_vocab.itos[pred])\n","        x = torch.tensor([[pred]])\n","\n","    return \" \".join(result)"],"metadata":{"id":"QDRHAO7-x4sm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(translate(\"input1\"))\n","print(translate(\"input2\"))\n","print(translate(\"input3\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RuMOUHG3x7WW","executionInfo":{"status":"ok","timestamp":1769506276388,"user_tz":-330,"elapsed":84,"user":{"displayName":"Alby J M","userId":"03291698004133179097"}},"outputId":"570cb524-28e2-474d-ea64-9c6a7b559438"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["translated_output1\n","translated_output2\n","translated_output3\n"]}]}]}