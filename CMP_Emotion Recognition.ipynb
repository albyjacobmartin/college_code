{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOvgelzODEFRlyCmyhc1lxP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pEivvEK0Rt2V","executionInfo":{"status":"ok","timestamp":1769193749718,"user_tz":-330,"elapsed":4807,"user":{"displayName":"Alby J M","userId":"03291698004133179097"}},"outputId":"ff645a42-8fa8-4cfe-d9bd-aa5c4d76e1ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using Colab cache for faster access to the 'fer2013' dataset.\n","Path to dataset files: /kaggle/input/fer2013\n"]}],"source":["import kagglehub\n","\n","path = kagglehub.dataset_download(\"msambare/fer2013\")\n","print(\"Path to dataset files:\", path)"]},{"cell_type":"code","source":["from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader\n","\n","train_tfms = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=3),\n","    transforms.Resize((224,224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(10),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406],\n","                         [0.229, 0.224, 0.225])\n","])\n","\n","val_tfms = transforms.Compose([\n","    transforms.Grayscale(num_output_channels=3),\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.485, 0.456, 0.406],\n","                         [0.229, 0.224, 0.225])\n","])"],"metadata":{"id":"BPQNpu89bGH5","executionInfo":{"status":"ok","timestamp":1769193766240,"user_tz":-330,"elapsed":14072,"user":{"displayName":"Alby J M","userId":"03291698004133179097"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["train_ds = ImageFolder('/kaggle/input/fer2013/train', transform=train_tfms)\n","val_ds   = ImageFolder('/kaggle/input/fer2013/test',  transform=val_tfms)\n","\n","train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\n","val_loader   = DataLoader(val_ds, batch_size=64, shuffle=False, num_workers=2)\n","\n","print(\"Classes:\", train_ds.classes)\n","print(\"Train size:\", len(train_ds))\n","print(\"Val size:\", len(val_ds))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c52yztLvflq9","executionInfo":{"status":"ok","timestamp":1769193825326,"user_tz":-330,"elapsed":44388,"user":{"displayName":"Alby J M","userId":"03291698004133179097"}},"outputId":"0105adfc-6d72-44c8-981b-65fc9763965d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n","Train size: 28709\n","Val size: 7178\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Device:\", device)\n","\n","# Load pretrained ResNet18\n","model = models.resnet18(pretrained=True)\n","\n","# Freeze backbone\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","# Replace classifier head\n","num_features = model.fc.in_features\n","model.fc = nn.Linear(num_features, 7)  # 7 emotions\n","\n","model = model.to(device)\n","\n","# Loss and optimizer (only train head)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n","\n","print(model.fc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"94DONLdAf3pf","executionInfo":{"status":"ok","timestamp":1769193829380,"user_tz":-330,"elapsed":888,"user":{"displayName":"Alby J M","userId":"03291698004133179097"}},"outputId":"2a1f04c9-120c-4366-f80c-888fcd61f8de"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 44.7M/44.7M [00:00<00:00, 180MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Linear(in_features=512, out_features=7, bias=True)\n"]}]},{"cell_type":"code","source":["num_epochs = 5\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for imgs, labels in train_loader:\n","        imgs, labels = imgs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(imgs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        preds = outputs.argmax(1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","    train_acc = correct / total\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n","          f\"Loss: {running_loss:.2f} \"\n","          f\"Train Acc: {train_acc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZeeHM3Qff4Ai","outputId":"bddad54f-d496-41a9-cd4b-d4aed8f435b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/5] Loss: 732.35 Train Acc: 0.3583\n","Epoch [2/5] Loss: 688.16 Train Acc: 0.4071\n","Epoch [3/5] Loss: 675.92 Train Acc: 0.4205\n","Epoch [4/5] Loss: 673.90 Train Acc: 0.4203\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, classification_report\n","import numpy as np\n","\n","model.eval()\n","all_preds = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for imgs, labels in val_loader:\n","        imgs = imgs.to(device)\n","        outputs = model(imgs)\n","        preds = outputs.argmax(1).cpu().numpy()\n","\n","        all_preds.extend(preds)\n","        all_labels.extend(labels.numpy())\n","\n","cm = confusion_matrix(all_labels, all_preds)\n","\n","print(\"Confusion Matrix:\\n\", cm)\n","print(\"\\nClassification Report:\\n\")\n","print(classification_report(all_labels, all_preds, target_names=train_ds.classes))"],"metadata":{"id":"juQaYYQDgc-b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1769155644001,"user_tz":-330,"elapsed":25580,"user":{"displayName":"Alby J M","userId":"03291698004133179097"}},"outputId":"99600378-aa88-43a1-b97d-1a46242c5100"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Confusion Matrix:\n"," [[ 102    0  106  379  185  138   48]\n"," [   4    0   15   61   13   16    2]\n"," [  31    0  194  360  188  156   95]\n"," [  27    0   68 1359  182  107   31]\n"," [  17    0   85  377  570  144   40]\n"," [  34    0  135  440  241  373   24]\n"," [  16    0   97  152  115   23  428]]\n","\n","Classification Report:\n","\n","              precision    recall  f1-score   support\n","\n","       angry       0.44      0.11      0.17       958\n","     disgust       0.00      0.00      0.00       111\n","        fear       0.28      0.19      0.23      1024\n","       happy       0.43      0.77      0.55      1774\n","     neutral       0.38      0.46      0.42      1233\n","         sad       0.39      0.30      0.34      1247\n","    surprise       0.64      0.52      0.57       831\n","\n","    accuracy                           0.42      7178\n","   macro avg       0.37      0.33      0.33      7178\n","weighted avg       0.41      0.42      0.39      7178\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n","/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}]},{"cell_type":"code","source":["# Unfreeze layer4 (last ResNet block)\n","for name, param in model.named_parameters():\n","    if \"layer4\" in name:\n","        param.requires_grad = True"],"metadata":{"id":"4Rww5XH83AVY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=1e-4)"],"metadata":{"id":"9qILALA58Dap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 3\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0\n","    correct = 0\n","    total = 0\n","\n","    for imgs, labels in train_loader:\n","        imgs, labels = imgs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(imgs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        preds = outputs.argmax(1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","    train_acc = correct / total\n","    print(f\"[Fine-tune] Epoch {epoch+1} Train Acc: {train_acc:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ifCZXeK28D17","executionInfo":{"status":"ok","timestamp":1769156645453,"user_tz":-330,"elapsed":238512,"user":{"displayName":"Alby J M","userId":"03291698004133179097"}},"outputId":"68936c70-cbf4-456b-a891-1764cfb28fc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Fine-tune] Epoch 1 Train Acc: 0.5360\n","[Fine-tune] Epoch 2 Train Acc: 0.6134\n","[Fine-tune] Epoch 3 Train Acc: 0.6517\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix, classification_report\n","import numpy as np\n","\n","model.eval()\n","all_preds = []\n","all_labels = []\n","\n","with torch.no_grad():\n","    for imgs, labels in val_loader:\n","        imgs = imgs.to(device)\n","        outputs = model(imgs)\n","        preds = outputs.argmax(1).cpu().numpy()\n","\n","        all_preds.extend(preds)\n","        all_labels.extend(labels.numpy())\n","\n","cm = confusion_matrix(all_labels, all_preds)\n","\n","print(\"Confusion Matrix (After Fine-Tuning):\\n\", cm)\n","print(\"\\nClassification Report (After Fine-Tuning):\\n\")\n","print(classification_report(all_labels, all_preds, target_names=train_ds.classes))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RUFk-W7b8G2Y","executionInfo":{"status":"ok","timestamp":1769157177139,"user_tz":-330,"elapsed":21073,"user":{"displayName":"Alby J M","userId":"03291698004133179097"}},"outputId":"24a06e05-8770-4400-c434-5a7f942988d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Confusion Matrix (After Fine-Tuning):\n"," [[ 459   14   69   75  200  115   26]\n"," [  33   42    5   12    8    9    2]\n"," [ 104    3  294   65  266  172  120]\n"," [  24    0    8 1523  167   23   29]\n"," [  34    1   23   79  968  110   18]\n"," [ 101    2   74   91  432  526   21]\n"," [  20    0   25   61   99    9  617]]\n","\n","Classification Report (After Fine-Tuning):\n","\n","              precision    recall  f1-score   support\n","\n","       angry       0.59      0.48      0.53       958\n","     disgust       0.68      0.38      0.49       111\n","        fear       0.59      0.29      0.39      1024\n","       happy       0.80      0.86      0.83      1774\n","     neutral       0.45      0.79      0.57      1233\n","         sad       0.55      0.42      0.48      1247\n","    surprise       0.74      0.74      0.74       831\n","\n","    accuracy                           0.62      7178\n","   macro avg       0.63      0.56      0.57      7178\n","weighted avg       0.63      0.62      0.60      7178\n","\n"]}]}]}